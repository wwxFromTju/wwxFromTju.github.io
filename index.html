<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- DELETE THIS SCRIPT if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');
  </script>

  <title>Weixun Wang 王维埙</title>
  
  <meta name="author" content="Weixun Wang 王维埙">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Weixun Wang 王维埙</name>
              </p>
              <p>I’m a PhD student at <a href="http://scs.tju.edu.cn/plus/list.php?tid=3">Tianjin University </a> in Professor <a href="http://www.escience.cn/people/jianye/index.html;jsessionid=A90C80C8698AA0C206F72C6A1945DE44-n2">Jianye Hao</a>’s group, where I work on Multiagent (Deep) Reinforcement Learning.
              </p>
              <p>

              </p>
              <p style="text-align:center">
                <a href="mailto:wxwang@tju.edu.cn">Email</a> &nbsp/&nbsp
<!--                <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp-->
<!--                <a href="data/JonBarron-bio.txt">Biography</a> &nbsp/&nbsp-->
                <a href="https://scholar.google.com.hk/citations?user=pG1-T4QAAAAJ&hl=en">Google Scholar</a>
<!--                <a href="http://www.linkedin.com/in/jonathanbarron/"> LinkedIn </a>-->
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/weixunwang.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/weixunwang.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I have an interest in using deep reinforcement learning in multi-agent systems. I believe that MAS (Multi-Agent) is a more realistic description of the (large) problem in the real world. I also believe that deep reinforcement learning can solve more complex practical problems in the MAS field.
              </p>
            </td>
          </tr>
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
          <tr onmouseout="dpzlearn_stop()" onmouseover="dpzlearn_start()">
            <td style="padding:20px;width:20%;vertical-align:middle">
              <img src='images/BiPaRs.png' height="150" width="200">
            </td>
            <td style="padding:25px;width:75%;vertical-align:middle">
              <a href="xxx">
                <papertitle>Learning to Utilize Shaping Rewards: A New Approach of Reward Shaping</papertitle>
              </a>
              <br>
              Yujing Hu, <strong>Weixun Wang</strong>, Hangtian Jia, Yixiang Wang, Yingfeng Chen, Jianye Hao, Feng Wu, Changjie Fan
              <br>
              <br> NeurIPS 2021
              <br>
              <p></p>
              <p> We formulate the utilization of shaping rewards as a bi-level optimization problem, where the lower level is to optimize policy using the shaping rewards and the upper level is to optimize a parameterized shaping weight function for true reward maximization. We formally derive the gradient of the expected true reward with respect to the shaping weight function parameters and accordingly propose three learning algorithms based on different assumptions. Experiments in sparse-reward cartpole and MuJoCo environments show that our algorithms can fully exploit beneficial shaping rewards, and meanwhile ignore unbeneficial shaping rewards or even transform them into beneficial ones.
              </p>
            </td>
          </tr>


          <tr onmouseout="dpzlearn_stop()" onmouseover="dpzlearn_start()">
            <td style="padding:20px;width:20%;vertical-align:middle">
              <img src='images/MAOPT.png' height="150" width="200">
            </td>
            <td style="padding:25px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2002.08030">
                <papertitle>Learning When to Transfer among Agents: An Efficient Multiagent Transfer Learning Framework</papertitle>
              </a>
              <br>
              Tianpei Yang*(Equal contribution), <strong>Weixun Wang*(Equal contribution)</strong>, Hongyao Tang*(Equal contribution), Jianye Hao, Zhaopeng Meng, Wulong Liu, Yujing Hu, Yingfeng Chen
              <br>
              <br> Arxiv
              <br>
              <p></p>
              <p> Our framework learns when and what advice to give to each agent and when to terminate it by modeling multi-agent transfer as the option learning problem. We also propose a novel option learning algorithm, named as the Successor Representation Option (SRO) learning that decouples the dynamics of the environment from the rewards to learn the option-value function under each agent’s preference.
              </p>
            </td>
          </tr>


          <tr onmouseout="dpzlearn_stop()" onmouseover="dpzlearn_start()">
            <td style="padding:20px;width:20%;vertical-align:middle">
              <img src='images/mc-GNN.png' height="150" width="200">
            </td>
            <td style="padding:25px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2002.08030">
                <papertitle>Learning to Accelerate Heuristic Searching for Large-Scale Maximum Weighted b-Matching Problems in Online Advertising</papertitle>
              </a>
              <br>
              Xiaotian Hao, Junqi Jin, Jin Li, <strong>Weixun Wang</strong>, Yi Ma, Jianye Hao, Zhenzhe Zheng, Han Li, Jian Xu, Kun Gai
              <br>
              <br> IJCAI 2020
              <br>
              <p></p>
              <p> We propose NeuSearcher which leverage the knowledge learned from previously instances to solve new problem instances. Specifically, we design a multichannel graph neural network to predict the threshold of the matched edges, by which the search region could be significantly reduced. We further propose a parallel heuristic search algorithm to iteratively improve the solution quality until convergence. Experiments on both open and industrial datasets demonstrate that NeuSearcher can speed up 2 to 3 times while achieving exactly the same matching solution compared with the state-of-the-art approximation approaches.
              </p>
            </td>
          </tr>


          <tr onmouseout="dpzlearn_stop()" onmouseover="dpzlearn_start()">
            <td style="padding:20px;width:20%;vertical-align:middle">
              <img src='images/KoGuN.png' height="150" width="200">
            </td>
            <td style="padding:25px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2002.07418v1">
                <papertitle>KoGuN: Accelerating Deep Reinforcement Learning via Integrating Human Suboptimal Knowledge</papertitle>
              </a>
              <br>
              Peng Zhang, Jianye Hao, <strong>Weixun Wang</strong>, Hongyao Tang, Yi Ma, Yihai Duan, Yan Zheng
              <br>
              <br> IJCAI 2020
              <br>
              <p></p>
              <p>We propose knowledge guided policy network (KoGuN), a novel framework that combines human prior suboptimal knowledge with reinforcement learning. Our framework consists of a fuzzy rule controller to represent human knowledge and a refine module to fine-tune suboptimal prior knowledge. The proposed framework is end-to-end and can be combined with existing policy-based reinforcement learning algorithm. 
              </p>
            </td>
          </tr>


          <tr onmouseout="dpzlearn_stop()" onmouseover="dpzlearn_start()">
            <td style="padding:20px;width:20%;vertical-align:middle">
              <img src='images/ARN.png' height="150" width="200">
            </td>
            <td style="padding:25px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=ryg48p4tPH">
                <papertitle>Action Semantics Network: Considering the Effects of Actions in Multiagent Systems</papertitle>
              </a>
              <br>
              Weixun Wang, Tianpei Yang Yong Liu, Jianye Hao, Xiaotian Hao, Yujing Hu, Yingfeng Chen, Changjie Fan, Yang Gao
              <br>
              <br> ICLR 2020
              <br>
              <p></p>
              <p>In this paper, we propose a novel network architecture, named Action Semantics Network (ASN), that explicitly represents such action semantics between agents. ASN characterizes different actions' influence on other agents using neural networks based on the action semantics between agents. 
              </p>
            </td>
          </tr>


          <tr onmouseout="dpzlearn_stop()" onmouseover="dpzlearn_start()">
            <td style="padding:20px;width:20%;vertical-align:middle">
              <img src='images/PTF.png' height="150" width="200">
            </td>
            <td style="padding:25px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=ryg48p4tPH">
                <papertitle>Efficient Deep Reinforcement Learning through Policy Transfer</papertitle>
              </a>
              <br>
              Tianpei Yang, Jianye Hao, Zhaopeng Meng, Zongzhang Zhang, <strong>Weixun Wang</strong>, Yujing Hu, Yingfeng Chen, Changjie Fan, Zhaodong Wang, Jiajie Peng
              <br>
              <br> AAMAS 2020 (abstract) + IJCAI 2020
              <br>
              <p></p>
              <p>We propose a novel Policy Transfer Framework (PTF) to accelerate RL by taking advantage of this idea. Our framework learns when and which source policy is the best to reuse for the target policy and when to terminate it by modeling multi-policy transfer as the option learning problem.  
              </p>
            </td>
          </tr>


          <tr onmouseout="dpzlearn_stop()" onmouseover="dpzlearn_start()">
            <td style="padding:20px;width:20%;vertical-align:middle">
              <img src='images/DyMA_CL.png' height="150" width="200">
            </td>
            <td style="padding:25px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1909.02790">
                <papertitle>From Few to More: Large-scale Dynamic Multiagent Curriculum Learning</papertitle>
              </a>
              <br>
              Weixun Wang, Tianpei Yang, Yong Liu, Jianye Hao, Xiaotian Hao, Yujing Hu, Yingfeng Chen, Changjie Fan, Yang Gao
              <br>
              <br> AAAI 2020
              <br>
              <p></p>
              <p> In this paper, we design a novel Dynamic Multiagent Curriculum Learning (DyMA-CL) to solve large-scale problems by starting from learning on a multiagent scenario with a small size and progressively increasing the number of agents. We propose three transfer mechanisms across curricula to accelerate the learning process. Moreover, due to the fact that the state dimension varies across curricula,, and existing network structures cannot be applied in such a transfer setting since their network input sizes are fixed. Therefore, we design a novel network structure called Dynamic Agent-number Network (DyAN) to handle the dynamic size of the network input.
              </p>
            </td>
          </tr>

          <tr onmouseout="dpzlearn_stop()" onmouseover="dpzlearn_start()">
            <td style="padding:20px;width:20%;vertical-align:middle">
              <img src='images/G2ANet.png' height="150" width="200">
            </td>
            <td style="padding:25px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1911.10715">
                <papertitle>Multi-Agent Game Abstraction via Graph Attention Neural Network</papertitle>
              </a>
              <br>
              Yong Liu*(Equal contribution),  <strong>Weixun Wang*(Equal contribution)</strong>, Yujing Hu, Jianye Hao, Xingguo Chen, Yang Gao
              <br>
              <br> AAAI 2020
              <br>
              <p></p>
              <p> In this paper, we model the relationship between agents by a complete graph and purpose a novel game abstraction mechanism based on two-stage attention network (G2ANet), which can indicate whether there is an interaction between two agents and the importance of the interaction. We integrate this detection mechanism into graph neural network-based multi-agent reinforcement learning for conducting game abstraction and propose two novel learning algorithms GA-Comm and GA-AC. We conduct experiments in Traffic Junction and Predator-Prey. The results indicate that the proposed methods can simplify the learning process and meanwhile get better asymptotic performance compared with state-of-the-art algorithms.
              </p>
            </td>
          </tr>


          <tr onmouseout="dpzlearn_stop()" onmouseover="dpzlearn_start()">
            <td style="padding:20px;width:20%;vertical-align:middle">
              <img src='images/L2A.png' height="150" width="200">
            </td>
            <td style="padding:25px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1809.03149">
                <papertitle>Learning Adaptive Display Exposure for Real-Time Advertising</papertitle>
              </a>
              <br>
              Weixun Wang, Junqi Jin, Jianye Hao, Chunjie Chen, Chuan Yu, Weinan Zhang, Jun Wang, Xiaotian Hao, Yixi Wang, Han Li, Jian Xu, Kun Gai
              <br>
              <br> CIKM 2019
              <br>
              <p></p>
              <p>In this paper, we investigate the problem of advertising with adaptive exposure, in which the number of ad slots and their locations can dynamically change over time based on their relative scores with recommendation products.
              </p>
            </td>
          </tr>

          <tr onmouseout="dpzlearn_stop()" onmouseover="dpzlearn_start()">
            <td style="padding:20px;width:20%;vertical-align:middle">
                <img src='images/GASIL.png' height="150" width="200">
            </td>
            <td style="padding:25px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1909.11468">
                <papertitle>Independent Generative Adversarial Self-Imitation Learning in Cooperative Multiagent Systems</papertitle>
              </a>
              <br>
              Xiaotian Hao *(Equal contribution),
              <strong>Weixun Wang *(Equal contribution), </strong>
              Jianye Hao,
              Yaodong Yang
              <br>
              <br> AAMAS 2019
              <br>
              <p></p>
              <p>The first to combine self imitation learning with GAIL and propose a novel framework IGASIL to address the multiagent coordination problems.
              </p>
            </td>
          </tr>

          <tr onmouseout="dpzlearn_stop()" onmouseover="dpzlearn_start()">
            <td style="padding:18px;width:20%;vertical-align:middle">
              <img src='images/SPD.png' height="170" width="210">
            </td>
            <td style="padding:25px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1803.00162">
                <papertitle>Towards Cooperation in Sequential Prisoner's Dilemmas: a Deep Multiagent Reinforcement Learning Approach</papertitle>
              </a>
              <br>
              Weixun Wang, Jianye Hao, Yixi Wang, Matthew Taylor
              <br>
              <br> AAMAS 2018 Workshop ALA, DAI 2019 (Best Paper Award)
              <br>
              <p></p>
              <p> In this work, we propose a deep multiagent reinforcement learning approach that investigates the evolution of mutual cooperation in SPD games.
              </p>
            </td>
          </tr>

        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
