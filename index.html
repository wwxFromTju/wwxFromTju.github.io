<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-7580334-2');
  </script>

  <title>Weixun Wang çŽ‹ç»´åŸ™</title>
  
  <meta name="author" content="Weixun Wang çŽ‹ç»´åŸ™">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&display=swap" rel="stylesheet">
  <style>
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }
    
    body {
      font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
      background: #0a0a0a;
      color: #e0e0e0;
      line-height: 1.6;
      overflow-x: hidden;
    }
    
    /* Animated gradient background */
    .bg-gradient {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      z-index: -1;
      background: linear-gradient(135deg, #0a0a0a 0%, #1a1a2e 50%, #16213e 100%);
    }
    
    .bg-gradient::before {
      content: '';
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      background: radial-gradient(circle at 20% 50%, rgba(120, 119, 198, 0.1) 0%, transparent 50%),
                  radial-gradient(circle at 80% 80%, rgba(99, 102, 241, 0.1) 0%, transparent 50%);
      animation: pulse 8s ease-in-out infinite;
    }
    
    @keyframes pulse {
      0%, 100% { opacity: 0.5; }
      50% { opacity: 1; }
    }
    
    .container {
      max-width: 1200px;
      margin: 0 auto;
      padding: 40px 20px;
      position: relative;
      z-index: 1;
    }
    
    /* Header Section */
    .header {
      display: grid;
      grid-template-columns: 1fr 300px;
      gap: 60px;
      align-items: center;
      padding: 80px 20px 60px;
      margin-bottom: 60px;
    }
    
    .header-content {
      order: 1;
    }
    
    .header-badge {
      display: inline-block;
      padding: 8px 20px;
      background: rgba(99, 102, 241, 0.1);
      border: 1px solid rgba(99, 102, 241, 0.3);
      border-radius: 50px;
      color: #818cf8;
      font-size: 14px;
      font-weight: 500;
      margin-bottom: 24px;
      letter-spacing: 0.5px;
    }
    
    .header h1 {
      font-size: 56px;
      font-weight: 800;
      background: linear-gradient(135deg, #818cf8 0%, #c084fc 100%);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
      margin-bottom: 16px;
      letter-spacing: -2px;
      line-height: 1.1;
    }
    
    .header .subtitle {
      font-size: 20px;
      color: #9ca3af;
      font-weight: 400;
      margin-bottom: 32px;
    }
    
    .header .bio {
      font-size: 16px;
      color: #d1d5db;
      line-height: 1.8;
      margin-bottom: 32px;
    }
    
    .header .bio a {
      color: #818cf8;
      text-decoration: none;
      transition: color 0.3s ease;
      border-bottom: 1px solid rgba(129, 140, 248, 0.3);
    }
    
    .header .bio a:hover {
      color: #c084fc;
      border-bottom-color: rgba(192, 132, 252, 0.5);
    }
    
    .header-links {
      display: flex;
      gap: 16px;
      flex-wrap: wrap;
    }
    
    .header-link {
      display: inline-flex;
      align-items: center;
      padding: 10px 20px;
      background: rgba(99, 102, 241, 0.1);
      border: 1px solid rgba(99, 102, 241, 0.3);
      border-radius: 12px;
      color: #818cf8;
      text-decoration: none;
      font-weight: 500;
      font-size: 14px;
      transition: all 0.3s ease;
    }
    
    .header-link:hover {
      background: rgba(99, 102, 241, 0.2);
      border-color: rgba(99, 102, 241, 0.5);
      transform: translateY(-2px);
    }
    
    .header-image {
      order: 2;
      position: relative;
    }
    
    .profile-wrapper {
      position: relative;
      border-radius: 24px;
      overflow: hidden;
      transition: all 0.4s ease;
    }
    
    .profile-wrapper:hover {
      transform: scale(1.05);
    }
    
    .profile-wrapper img {
      width: 100%;
      height: auto;
      display: block;
    }
    
    /* Section Title */
    .section-title {
      font-size: 36px;
      font-weight: 700;
      color: #f3f4f6;
      margin: 60px 0 20px;
      position: relative;
    }
    
    .section-title::after {
      content: '';
      display: block;
      width: 60px;
      height: 4px;
      background: linear-gradient(90deg, #818cf8, #c084fc);
      margin-top: 16px;
      border-radius: 2px;
    }
    
    .section-description {
      color: #9ca3af;
      font-size: 16px;
      line-height: 1.8;
      margin-bottom: 40px;
      max-width: 800px;
    }
    
    /* Research Section */
    .research-intro {
      background: rgba(17, 24, 39, 0.5);
      border: 1px solid rgba(99, 102, 241, 0.2);
      border-radius: 24px;
      padding: 40px;
      margin-bottom: 60px;
      backdrop-filter: blur(10px);
    }
    
    .research-intro p {
      font-size: 17px;
      line-height: 1.8;
      color: #d1d5db;
      margin: 0;
    }
    
    /* Paper Card */
    .papers-grid {
      display: grid;
      gap: 24px;
      margin-top: 40px;
    }
    
    .paper-card {
      background: rgba(17, 24, 39, 0.6);
      border: 1px solid rgba(99, 102, 241, 0.2);
      border-radius: 20px;
      padding: 32px;
      transition: all 0.4s cubic-bezier(0.4, 0, 0.2, 1);
      position: relative;
      overflow: hidden;
      backdrop-filter: blur(10px);
      display: grid;
      grid-template-columns: 220px 1fr;
      gap: 32px;
      align-items: start;
    }
    
    .paper-card::before {
      content: '';
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      background: linear-gradient(135deg, rgba(99, 102, 241, 0.1) 0%, transparent 100%);
      opacity: 0;
      transition: opacity 0.4s ease;
    }
    
    .paper-card:hover {
      transform: translateY(-8px);
      border-color: rgba(99, 102, 241, 0.5);
      box-shadow: 0 20px 60px rgba(99, 102, 241, 0.2);
    }
    
    .paper-card:hover::before {
      opacity: 1;
    }
    
    .paper-image {
      position: relative;
      z-index: 1;
    }
    
    .paper-image img {
      width: 100%;
      height: auto;
      border-radius: 12px;
      border: 1px solid rgba(99, 102, 241, 0.2);
    }
    
    .paper-content {
      position: relative;
      z-index: 1;
    }
    
    .paper-title {
      font-size: 20px;
      font-weight: 600;
      line-height: 1.4;
      margin-bottom: 12px;
    }
    
    .paper-title a {
      color: #f3f4f6;
      text-decoration: none;
      transition: color 0.3s ease;
    }
    
    .paper-title a:hover {
      color: #818cf8;
    }
    
    .paper-authors {
      color: #9ca3af;
      font-size: 14px;
      line-height: 1.6;
      margin-bottom: 8px;
    }
    
    .paper-authors strong {
      color: #d1d5db;
      font-weight: 600;
    }
    
    .paper-venue {
      display: inline-block;
      padding: 4px 12px;
      background: rgba(99, 102, 241, 0.15);
      border-radius: 6px;
      color: #818cf8;
      font-size: 13px;
      font-weight: 600;
      margin: 8px 0 16px 0;
    }
    
    .paper-summary {
      color: #9ca3af;
      font-size: 15px;
      line-height: 1.7;
      margin-top: 16px;
    }
    
    /* Footer */
    .footer {
      text-align: center;
      padding: 60px 20px 40px;
      margin-top: 80px;
      border-top: 1px solid rgba(99, 102, 241, 0.2);
      color: #6b7280;
      font-size: 14px;
    }
    
    /* Responsive */
    @media (max-width: 768px) {
      .header {
        grid-template-columns: 1fr;
        gap: 40px;
      }
      
      .header-content {
        order: 2;
      }
      
      .header-image {
        order: 1;
      }
      
      .header h1 {
        font-size: 40px;
      }
      
      .header .subtitle {
        font-size: 18px;
      }
      
      .paper-card {
        grid-template-columns: 1fr;
        gap: 20px;
        padding: 24px;
      }
      
      .section-title {
        font-size: 28px;
      }
    }
  </style>
</head>

<body>
  <div class="bg-gradient"></div>
  
  <div class="container">
    <!-- Header -->
    <header class="header">
      <div class="header-content">
        <div class="header-badge">ðŸ¤– RL Researcher @ Alibaba</div>
        <h1>Weixun Wang<br>çŽ‹ç»´åŸ™</h1>
        <p class="subtitle">Reinforcement Learning & Agentic AI</p>
        <p class="bio">
          I am currently a reinforcement learning researcher at Alibaba, where I focus on applying RL to enhance LLM reasoning capabilities and develop agentic AI systems. My research explores how reinforcement learning can improve the decision-making and problem-solving abilities of large language models in complex, multi-step tasks. Previously, I worked at NetEase Games Fuxi AI Lab, where I applied reinforcement learning throughout the game development lifecycle. I completed my Ph.D. at Tianjin University under the supervision of <a href="http://www.escience.cn/people/jianye/index.html;jsessionid=A90C80C8698AA0C206F72C6A1945DE44-n2">Professor Jianye Hao</a>. I am passionate about the transformative potential of (multi-agent) reinforcement learning and believe it will continue to reshape our world in profound ways.
        </p>
        <div class="header-links">
          <a href="mailto:wxwang@tju.edu.cn" class="header-link">ðŸ“§ Email</a>
          <a href="https://scholar.google.com.hk/citations?user=pG1-T4QAAAAJ&hl=en" class="header-link">ðŸŽ“ Google Scholar</a>
          <a href="roll_team.html" class="header-link">ðŸš€ ROLL Team</a>
        </div>
      </div>
      <div class="header-image">
        <div class="profile-wrapper">
          <img src="images/weixunwang.png" alt="Weixun Wang">
        </div>
      </div>
    </header>
    
    <!-- Research Section -->
    <section>
      <h2 class="section-title">Research</h2>
      <div class="research-intro">
        <p>
          I have an interest in using deep reinforcement learning in multi-agent systems. I believe that MAS (Multi-Agent) is a more realistic description of the (large) problem in the real world. I also believe that deep reinforcement learning can solve more complex practical problems in the MAS field.
        </p>
      </div>
    </section>
    
    <!-- Publications Section -->
    <section>
      <h2 class="section-title">Selected Publications</h2>
      
      <div class="papers-grid">
        <!-- Paper 1: N+ -->
        <div class="paper-card">
          <div class="paper-image">
            <img src="images/N+.png" alt="N+ Paper">
          </div>
          <div class="paper-content">
            <div class="paper-title">
              <a href="https://arxiv.org/pdf/2403.17031.pdf" target="_blank">The N+ Implementation Details of RLHF with PPO: A Case Study on TL;DR Summarization</a>
            </div>
            <div class="paper-authors">
              Shengyi Huang, Michael Noukhovitch, Arian Hosseini, Kashif Rasul, <strong>Weixun Wang</strong>, Lewis Tunstall
            </div>
            <div class="paper-venue">COLM 2024</div>
            <p class="paper-summary">
              This work is the first to openly reproduce the Reinforcement Learning from Human Feedback (RLHF) scaling behaviors reported in OpenAI's seminal TL;DR summarization work. We create an RLHF pipeline from scratch, enumerate over 20 key implementation details, and share key insights during the reproduction.
            </p>
          </div>
        </div>
        
        <!-- Paper 2: MARLlib -->
        <div class="paper-card">
          <div class="paper-image">
            <img src="images/marrlib.png" alt="MARLlib">
          </div>
          <div class="paper-content">
            <div class="paper-title">
              <a href="https://jmlr.org/papers/v24/23-0378.html" target="_blank">MARLlib: A Scalable and Efficient Multi-agent Reinforcement Learning Library</a>
            </div>
            <div class="paper-authors">
              Siyi Hu, Yifan Zhong, Minquan Gao, <strong>Weixun Wang</strong>, Hao Dong, Xiaodan Liang, Zhihui Li, Xiaojun Chang, Yaodong Yang
            </div>
            <div class="paper-venue">JMLR 2023</div>
            <p class="paper-summary">
              We present MARLlib, a library designed to address the challenge of fast and compatible development for multi-agent tasks and algorithm combinations. The library features a standardized multi-agent environment wrapper, agent-level algorithm implementation, and flexible policy mapping strategy.
            </p>
          </div>
        </div>
        
        <!-- Paper 3: RE-QMIX -->
        <div class="paper-card">
          <div class="paper-image">
            <img src="images/RE_QMIX.png" alt="RE-QMIX">
          </div>
          <div class="paper-content">
            <div class="paper-title">
              <a href="https://iclr-blogposts.github.io/2023/blog/2023/riit/" target="_blank">Rethinking the Implementation Tricks and Monotonicity Constraint in Cooperative Multi-agent Reinforcement Learning</a>
            </div>
            <div class="paper-authors">
              Jian Hu, Siying Wang, Siyang Jiang, <strong>Weixun Wang</strong>
            </div>
            <div class="paper-venue">ICLR 2023 Blog</div>
            <p class="paper-summary">
              We found that by improving the implementation techniques of QMIX we can enable it to achieve state-of-the-art on the StarCraft Multi-Agent Challenge (SMAC) testbed. We also explored the key factor of the monotonicity constraint of QMIX.
            </p>
          </div>
        </div>
        
        <!-- Paper 4: LA-QTransformer -->
        <div class="paper-card">
          <div class="paper-image">
            <img src="images/LA-QTransformer.png" alt="LA-QTransformer">
          </div>
          <div class="paper-content">
            <div class="paper-title">
              <a href="https://ieeexplore.ieee.org/abstract/document/10114566" target="_blank">Cooperative Multi-Agent Transfer Learning with Coalition Pattern Decomposition</a>
            </div>
            <div class="paper-authors">
              Tianze Zhou, Fubiao Zhang, Kun Shao, Zipeng Dai, Kai Li, Wenhan Huang, <strong>Weixun Wang</strong>, Bin Wang, Dong Li, Wulong Liu, Jianye Hao
            </div>
            <div class="paper-venue">Transactions on Games 2023</div>
            <p class="paper-summary">
              We propose a level-adaptive MARL framework called "LA-QTransformer", to realize the knowledge transfer on the coordination level via efficiently decomposing the agent coordination into multi-level coalition patterns for different agents.
            </p>
          </div>
        </div>
        
        <!-- Paper 5: PORTAL -->
        <div class="paper-card">
          <div class="paper-image">
            <img src="images/PORTAL.png" alt="PORTAL">
          </div>
          <div class="paper-content">
            <div class="paper-title">
              <a href="https://dl.acm.org/doi/abs/10.5555/3545946.3598967" target="_blank">PORTAL: Automatic Curricula Generation for Multiagent Reinforcement Learning</a>
            </div>
            <div class="paper-authors">
              Jizhou Wu, Tianpei Yang, Xiaotian Hao, Jianye Hao, Yan Zheng, <strong>Weixun Wang</strong>, Matthew E. Taylor
            </div>
            <div class="paper-venue">AAMAS 2023</div>
            <p class="paper-summary">
              We propose a novel ACL framework, PORTAL, for MASs. PORTAL selects curricula based on task difficulty and similarity to the final task, enabling agents to master extremely hard cooperative tasks.
            </p>
          </div>
        </div>
        
        <!-- Paper 6: API -->
        <div class="paper-card">
          <div class="paper-image">
            <img src="images/API.png" alt="API">
          </div>
          <div class="paper-content">
            <div class="paper-title">
              <a href="https://arxiv.org/abs/2203.05285" target="_blank">Boosting Multiagent Reinforcement Learning via Permutation Invariant and Permutation Equivariant Networks</a>
            </div>
            <div class="paper-authors">
              Jianye Hao, Xiaotian Hao, Hangyu Mao, <strong>Weixun Wang</strong>, Yaodong Yang, Dong Li, Yan Zheng, Zhen Wang
            </div>
            <div class="paper-venue">ICLR 2023</div>
            <p class="paper-summary">
              We propose two novel designs to achieve permutation invariance. Empirical results on the SMAC benchmark show that the proposed method achieves 100% win-rates in almost all hard and super-hard scenarios.
            </p>
          </div>
        </div>
        
        <!-- Paper 7: Off-Beat -->
        <div class="paper-card">
          <div class="paper-image">
            <img src="images/off-beat.png" alt="Off-Beat">
          </div>
          <div class="paper-content">
            <div class="paper-title">
              <a href="https://arxiv.org/abs/2205.13718" target="_blank">Off-Beat Multi-Agent Reinforcement Learning</a>
            </div>
            <div class="paper-authors">
              Wei Qiu, <strong>Weixun Wang</strong>, Rundong Wang, Bo An, Yujing Hu, Svetlana Obraztsova, Zinovi Rabinovich, Jianye Hao, Yingfeng Chen, Changjie Fan
            </div>
            <div class="paper-venue">AAMAS 2023</div>
            <p class="paper-summary">
              We propose LeGEM, a novel episodic memory for model-free MARL algorithms. LeGEM boosts multi-agent learning by addressing the challenging temporal credit assignment problem raised by off-beat actions.
            </p>
          </div>
        </div>
        
        <!-- Paper 8: ATM -->
        <div class="paper-card">
          <div class="paper-image">
            <img src="images/ATM.png" alt="ATM">
          </div>
          <div class="paper-content">
            <div class="paper-title">
              <a href="http://wwxfromtju.github.io/" target="_blank">Transformer-based Working Memory for Multiagent Reinforcement Learning with Action Parsing</a>
            </div>
            <div class="paper-authors">
              Yaodong Yang, Guangyong Chen, <strong>Weixun Wang</strong>, Xiaotian Hao, Jianye Hao, Pheng-Ann Heng
            </div>
            <div class="paper-venue">NeurIPS 2022</div>
            <p class="paper-summary">
              We propose the Agent Transformer Memory (ATM) network with a transformer-based memory. ATM utilizes the transformer to enable the unified processing of the factored environmental entities and memory.
            </p>
          </div>
        </div>
        
        <!-- Paper 9: A2C-PPO -->
        <div class="paper-card">
          <div class="paper-image">
            <img src="images/A2C-PPO.png" alt="A2C-PPO">
          </div>
          <div class="paper-content">
            <div class="paper-title">
              <a href="https://arxiv.org/abs/2205.09123" target="_blank">A2C is a special case of PPO</a>
            </div>
            <div class="paper-authors">
              Shengyi Huang, Anssi Kanervisto, Antonin Raffin, <strong>Weixun Wang</strong>, Santiago OntaÃ±Ã³n, Rousslan Fernand Julien Dossa
            </div>
            <div class="paper-venue">arXiv</div>
            <p class="paper-summary">
              We show A2C is a special case of PPO. We present theoretical justifications and pseudocode analysis to demonstrate why, validated through empirical experiments.
            </p>
          </div>
        </div>
        
        <!-- Paper 10: Coach -->
        <div class="paper-card">
          <div class="paper-image">
            <img src="images/coach.png" alt="Coach">
          </div>
          <div class="paper-content">
            <div class="paper-title">
              <a href="https://arxiv.org/abs/2203.08454" target="_blank">Coach-assisted Multi-Agent Reinforcement Learning Framework for Unexpected Crashed Agents</a>
            </div>
            <div class="paper-authors">
              Jian Zhao, Youpeng Zhao, <strong>Weixun Wang</strong>, Mingyu Yang, Xunhan Hu, Wengang Zhou, Jianye Hao, Houqiang Li
            </div>
            <div class="paper-venue">arXiv</div>
            <p class="paper-summary">
              We propose a coach-assisted multi-agent reinforcement learning framework, which introduces a virtual coach agent to adjust the crash rate during training to enhance system robustness.
            </p>
          </div>
        </div>
        
        <!-- Paper 11: IRAT -->
        <div class="paper-card">
          <div class="paper-image">
            <img src="images/IRAT.png" alt="IRAT">
          </div>
          <div class="paper-content">
            <div class="paper-title">
              <a href="https://wwxfromtju.github.io" target="_blank">Individual Reward Assisted Multi-Agent Reinforcement Learning</a>
            </div>
            <div class="paper-authors">
              Li Wang, Yupeng Zhang, Yujing Hu, <strong>Weixun Wang</strong>, Chongjie Zhang, Yang Gao, Jianye Hao, Tangjie Lv, Changjie Fan
            </div>
            <div class="paper-venue">ICML 2022</div>
            <p class="paper-summary">
              We propose Individual Reward Assisted Team Policy Learning (IRAT), which learns two policies for each agent from dense individual reward and sparse team reward with discrepancy constraints.
            </p>
          </div>
        </div>
        
        <!-- Paper 12: 37 PPO -->
        <div class="paper-card">
          <div class="paper-image">
            <img src="images/37-ppo.png" alt="37 PPO">
          </div>
          <div class="paper-content">
            <div class="paper-title">
              <a href="https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/" target="_blank">The 37 Implementation Details of Proximal Policy Optimization</a>
            </div>
            <div class="paper-authors">
              Shengyi Huang, Rousslan Fernand Julien Dossa, Antonin Raffin, Anssi Kanervisto, <strong>Weixun Wang</strong>
            </div>
            <div class="paper-venue">ICLR 2022 Blog</div>
            <p class="paper-summary">
              This blog post focuses on delivering a thorough reproduction of PPO, aggregating, documenting, and cataloging its most salient implementation details to help people understand PPO faster and better.
            </p>
          </div>
        </div>
        
        <!-- Paper 13: MAOPT -->
        <div class="paper-card">
          <div class="paper-image">
            <img src="images/MAOPT.png" alt="MAOPT">
          </div>
          <div class="paper-content">
            <div class="paper-title">
              <a href="https://arxiv.org/abs/2002.08030" target="_blank">An Efficient Transfer Learning Framework for Multiagent Reinforcement Learning</a>
            </div>
            <div class="paper-authors">
              Tianpei Yang* (Equal), <strong>Weixun Wang*</strong> (Equal), Hongyao Tang* (Equal), Jianye Hao, Zhaopeng Meng, Hangyu Mao, Dong Li, Wulong Liu, Chengwei Zhang, Yujing Hu, Yingfeng Chen, Changjie Fan
            </div>
            <div class="paper-venue">NeurIPS 2021</div>
            <p class="paper-summary">
              We propose a novel Multiagent Policy Transfer Framework (MAPTF) to improve MARL efficiency by modeling multiagent policy transfer as the option learning problem.
            </p>
          </div>
        </div>
        
        <!-- Paper 14: BiPaRs -->
        <div class="paper-card">
          <div class="paper-image">
            <img src="images/BiPaRs.png" alt="BiPaRs">
          </div>
          <div class="paper-content">
            <div class="paper-title">
              <a href="https://papers.nips.cc/paper/2020/hash/b710915795b9e9c02cf10d6d2bdb688c-Abstract.html" target="_blank">Learning to Utilize Shaping Rewards: A New Approach of Reward Shaping</a>
            </div>
            <div class="paper-authors">
              Yujing Hu, <strong>Weixun Wang</strong>, Hangtian Jia, Yixiang Wang, Yingfeng Chen, Jianye Hao, Feng Wu, Changjie Fan
            </div>
            <div class="paper-venue">NeurIPS 2020</div>
            <p class="paper-summary">
              We formulate the utilization of shaping rewards as a bi-level optimization problem and propose three learning algorithms that can fully exploit beneficial shaping rewards.
            </p>
          </div>
        </div>
        
        <!-- Paper 15: mc-GNN -->
        <div class="paper-card">
          <div class="paper-image">
            <img src="images/mc-GNN.png" alt="mc-GNN">
          </div>
          <div class="paper-content">
            <div class="paper-title">
              <a href="https://arxiv.org/abs/2002.08030" target="_blank">Learning to Accelerate Heuristic Searching for Large-Scale Maximum Weighted b-Matching Problems in Online Advertising</a>
            </div>
            <div class="paper-authors">
              Xiaotian Hao, Junqi Jin, Jin Li, <strong>Weixun Wang</strong>, Yi Ma, Jianye Hao, Zhenzhe Zheng, Han Li, Jian Xu, Kun Gai
            </div>
            <div class="paper-venue">IJCAI 2020</div>
            <p class="paper-summary">
              We propose NeuSearcher which leverages knowledge learned from previous instances to solve new problem instances, achieving 2-3x speedup while maintaining solution quality.
            </p>
          </div>
        </div>
        
        <!-- Paper 16: KoGuN -->
        <div class="paper-card">
          <div class="paper-image">
            <img src="images/KoGuN.png" alt="KoGuN">
          </div>
          <div class="paper-content">
            <div class="paper-title">
              <a href="https://arxiv.org/abs/2002.07418v1" target="_blank">KoGuN: Accelerating Deep Reinforcement Learning via Integrating Human Suboptimal Knowledge</a>
            </div>
            <div class="paper-authors">
              Peng Zhang, Jianye Hao, <strong>Weixun Wang</strong>, Hongyao Tang, Yi Ma, Yihai Duan, Yan Zheng
            </div>
            <div class="paper-venue">IJCAI 2020</div>
            <p class="paper-summary">
              We propose knowledge guided policy network (KoGuN), a novel framework that combines human prior suboptimal knowledge with reinforcement learning through a fuzzy rule controller.
            </p>
          </div>
        </div>
        
        <!-- Paper 17: ARN -->
        <div class="paper-card">
          <div class="paper-image">
            <img src="images/ARN.png" alt="ARN">
          </div>
          <div class="paper-content">
            <div class="paper-title">
              <a href="https://openreview.net/forum?id=ryg48p4tPH" target="_blank">Action Semantics Network: Considering the Effects of Actions in Multiagent Systems</a>
            </div>
            <div class="paper-authors">
              <strong>Weixun Wang</strong>, Tianpei Yang, Yong Liu, Jianye Hao, Xiaotian Hao, Yujing Hu, Yingfeng Chen, Changjie Fan, Yang Gao
            </div>
            <div class="paper-venue">ICLR 2020</div>
            <p class="paper-summary">
              We propose Action Semantics Network (ASN), a novel network architecture that explicitly represents action semantics between agents using neural networks.
            </p>
          </div>
        </div>
        
        <!-- Paper 18: PTF -->
        <div class="paper-card">
          <div class="paper-image">
            <img src="images/PTF.png" alt="PTF">
          </div>
          <div class="paper-content">
            <div class="paper-title">
              <a href="https://openreview.net/forum?id=ryg48p4tPH" target="_blank">Efficient Deep Reinforcement Learning through Policy Transfer</a>
            </div>
            <div class="paper-authors">
              Tianpei Yang, Jianye Hao, Zhaopeng Meng, Zongzhang Zhang, <strong>Weixun Wang</strong>, Yujing Hu, Yingfeng Chen, Changjie Fan, Zhaodong Wang, Jiajie Peng
            </div>
            <div class="paper-venue">AAMAS 2020 (abstract) + IJCAI 2020</div>
            <p class="paper-summary">
              We propose a novel Policy Transfer Framework (PTF) to accelerate RL by taking advantage of this idea. Our framework learns when and which source policy is the best to reuse for the target policy.
            </p>
          </div>
        </div>
        
        <!-- Paper 19: DyMA-CL -->
        <div class="paper-card">
          <div class="paper-image">
            <img src="images/DyMA_CL.png" alt="DyMA-CL">
          </div>
          <div class="paper-content">
            <div class="paper-title">
              <a href="https://arxiv.org/abs/1909.02790" target="_blank">From Few to More: Large-scale Dynamic Multiagent Curriculum Learning</a>
            </div>
            <div class="paper-authors">
              <strong>Weixun Wang</strong>, Tianpei Yang, Yong Liu, Jianye Hao, Xiaotian Hao, Yujing Hu, Yingfeng Chen, Changjie Fan, Yang Gao
            </div>
            <div class="paper-venue">AAAI 2020</div>
            <p class="paper-summary">
              We design a novel Dynamic Multiagent Curriculum Learning (DyMA-CL) to solve large-scale problems by starting from learning on a multiagent scenario with a small size and progressively increasing the number of agents.
            </p>
          </div>
        </div>
        
        <!-- Paper 20: G2ANet -->
        <div class="paper-card">
          <div class="paper-image">
            <img src="images/G2ANet.png" alt="G2ANet">
          </div>
          <div class="paper-content">
            <div class="paper-title">
              <a href="https://arxiv.org/abs/1911.10715" target="_blank">Multi-Agent Game Abstraction via Graph Attention Neural Network</a>
            </div>
            <div class="paper-authors">
              Yong Liu* (Equal), <strong>Weixun Wang*</strong> (Equal), Yujing Hu, Jianye Hao, Xingguo Chen, Yang Gao
            </div>
            <div class="paper-venue">AAAI 2020</div>
            <p class="paper-summary">
              We model the relationship between agents by a complete graph and propose a novel game abstraction mechanism based on two-stage attention network (G2ANet), which can indicate whether there is an interaction between two agents.
            </p>
          </div>
        </div>
        
        <!-- Paper 21: L2A -->
        <div class="paper-card">
          <div class="paper-image">
            <img src="images/L2A.png" alt="L2A">
          </div>
          <div class="paper-content">
            <div class="paper-title">
              <a href="https://arxiv.org/abs/1809.03149" target="_blank">Learning Adaptive Display Exposure for Real-Time Advertising</a>
            </div>
            <div class="paper-authors">
              <strong>Weixun Wang</strong>, Junqi Jin, Jianye Hao, Chunjie Chen, Chuan Yu, Weinan Zhang, Jun Wang, Xiaotian Hao, Yixi Wang, Han Li, Jian Xu, Kun Gai
            </div>
            <div class="paper-venue">CIKM 2019</div>
            <p class="paper-summary">
              We investigate the problem of advertising with adaptive exposure, in which the number of ad slots and their locations can dynamically change over time based on their relative scores with recommendation products.
            </p>
          </div>
        </div>
        
        <!-- Paper 22: GASIL -->
        <div class="paper-card">
          <div class="paper-image">
            <img src="images/GASIL.png" alt="GASIL">
          </div>
          <div class="paper-content">
            <div class="paper-title">
              <a href="https://arxiv.org/abs/1909.11468" target="_blank">Independent Generative Adversarial Self-Imitation Learning in Cooperative Multiagent Systems</a>
            </div>
            <div class="paper-authors">
              Xiaotian Hao* (Equal), <strong>Weixun Wang*</strong> (Equal), Jianye Hao, Yaodong Yang
            </div>
            <div class="paper-venue">AAMAS 2019</div>
            <p class="paper-summary">
              The first to combine self imitation learning with GAIL and propose a novel framework IGASIL to address the multiagent coordination problems.
            </p>
          </div>
        </div>
        
        <!-- Paper 23: SPD -->
        <div class="paper-card">
          <div class="paper-image">
            <img src="images/SPD.png" alt="SPD">
          </div>
          <div class="paper-content">
            <div class="paper-title">
              <a href="https://arxiv.org/abs/1803.00162" target="_blank">Towards Cooperation in Sequential Prisoner's Dilemmas: a Deep Multiagent Reinforcement Learning Approach</a>
            </div>
            <div class="paper-authors">
              <strong>Weixun Wang</strong>, Jianye Hao, Yixi Wang, Matthew Taylor
            </div>
            <div class="paper-venue">AAMAS 2018 Workshop ALA, DAI 2019 (Best Paper Award)</div>
            <p class="paper-summary">
              In this work, we propose a deep multiagent reinforcement learning approach that investigates the evolution of mutual cooperation in SPD games.
            </p>
          </div>
        </div>
      </div>
    </section>
    
    <!-- Footer -->
    <footer class="footer">
      <p>Â© 2026 Weixun Wang. Powered by modern web design.</p>
    </footer>
  </div>
</body>
</html>